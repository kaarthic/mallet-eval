20100422 MALLET-EVAL PROJECT


GENERAL

This is a project for evaluating MALLET (MAchine Learning for 
LanguagE Toolkit). MALLET's binary and source codes are not included, 
you can check out them from this site:

    http://mallet.cs.umass.edu/

This distribution only contains sample data annotation information 
and scripts for converting, importing and evaluating. The articles
in the two corpora have not been included here for copyright reasons. 
That is why you need the the two cds for building the complete data sets.

We provide two sample corpora: Penn Treebank Sample (5% fragment of Penn
Treebank) and HIT CIR LTP Corpora Sample (10% fragemnt of the whole
Corpora)

    http://web.mit.edu/course/6/6.863/OldFiles/share/data/corpora/treebank/
    http://ir.hit.edu.cn/demo/ltp/Sharing_Plan.htm


BUILDING THE TRAIN AND TEST DATA FILES

In order to obtain the data files you need to perform three steps:

   1. Get a local copy of the mallet-eval repository with this command:
       
       hg clone https://mallet-eval.googlecode.com/hg/ mallet-eval 

   2. Set up $MALLET_HOME enviroment: export MALLET_HOME=/path/to/mallet/

   3. Train and test with provided Chunking, POS Tagging and Named Entity 
      Recognition data (chunking/ pos-tagging/ ner/)

   2a (Chunking) ./conlleval < chunking/conlleval.out

   2b (POS-Tagging) cd pos-tagging | ./verify.py   

   2c (Named Entity Recognition) ./chn-conlleval < ner/conlleval.out

and the results are:

   eng.testa: precision:  78.33%; recall:  65.23%; FB1:  71.18
   eng.testb: precision:  71.91%; recall:  50.90%; FB1:  59.61
   deu.testa: precision:  37.19%; recall:  26.07%; FB1:  30.65
   deu.testb: precision:  31.86%; recall:  28.89%; FB1:  30.30


DATA FORMAT

The data files contain one word per line. Empty lines have been used
for marking sentence boundaries and a line containing the keyword
-DOCSTART- has been added to the beginning of each article in order
to mark article boundaries. Each non-empty line contains the following 
tokens:

   1. the current word
   2. the lemma of the word (German only)
   3. the part-of-speech (POS) tag generated by a tagger
   4. the chunk tag generated by a text chunker
   5. the named entity tag given by human annotators

The tagger and chunker for English are roughly similar to the 
ones used in the memory-based shallow parser demo available at 
http://ilk.uvt.nl/  German POS and chunk information has been 
generated by the Treetagger from the University of Stuttgart:
http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/
In order to simulate a real natural language processing 
environment, the POS tags and chunk tags have not been checked. 
This means that they will contain errors. If you have access to 
annotation software with a performance that is superior to this, 
you may replace these tags by yours.

The chunk tags and the named entity tags use the IOB1 format. This 
means that in general words inside entity receive the tag I-TYPE
to denote that they are Inside an entity of type TYPE. Whenever
two entities of the same type immediately follow each other, the 
first word of the second entity will receive tag B-TYPE rather than
I-TYPE in order to show that a new entity starts at that word.

The raw data has the same format as the training and test material
but the final column has been ommitted. There are word lists for 
English (extracted from the training data), German (extracted from 
the training data), and Dutch in the directory lists. Probably you 
can use the Dutch person names (PER) for English data as well. Feel 
free to use any other external data sources that you might have 
access to.


Max Lv <lch@fudan.edu.cn>

